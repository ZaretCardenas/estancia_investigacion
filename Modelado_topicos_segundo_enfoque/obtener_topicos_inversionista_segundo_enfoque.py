# -*- coding: utf-8 -*-
"""Obtener_topicos_inversionista_Segundo_enfoque.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ja0QOSp52Pufro_HFf82NOC72EG1Fs4i
"""

!pip install pyLDAvis
!pip install keybert
!pip install bertopic

import pandas as pd
import numpy as np
import gensim
from gensim import corpora, models
from gensim.models.ldamulticore import LdaMulticore
import pyLDAvis.gensim
from nltk.corpus import stopwords
import string
from nltk.stem.wordnet import WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore')
from itertools import chain
from collections import defaultdict
import re
import os
import spacy
nlp = spacy.load('en_core_web_sm')
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from bertopic import BERTopic
import json
from keybert import KeyBERT

pd.set_option('display.max_colwidth', None)

import scipy
import gensim
from pydantic import VERSION
#import openai
import spacy
import keybert
import bertopic

#gets the NLTK's stopword list for English
stops = set(stopwords.words("english"))
exclude = set(string.punctuation)
lemma  = WordNetLemmatizer()
nltk.download('wordnet')
from nltk import SnowballStemmer
stemmer = SnowballStemmer('english')


#print('The openai version is {}.'.format(openai.__version__))
print('The scipy version is {}.'.format(scipy.__version__))
print('The pydantic version is {}.'.format(VERSION))
print('The gensim version is {}.'.format(gensim.__version__))
print('The bertopic version is {}.'.format(bertopic.__version__))
print('The keybert version is {}.'.format(keybert.__version__))
print('The spacy version is {}.'.format(spacy.__version__))
print('The pyLDAvis version is {}.'.format(pyLDAvis.__version__))
print('The numpy version is {}.'.format(np.__version__))

def preprocesing_raw_content_shark(content, show, dialoguesbygenshark):
  dialogues = defaultdict(list)

  # Pattern to match the speaker and text
  pattern = re.compile(r"\[(.*?)\]\n(.+?)(?=\[|$)", re.DOTALL)

  # Find all matches
  matches = pattern.findall(content)
  #print(matches)

  for speaker, content in matches:
    #print(speaker)
    #print(content)
    speaker_type = re.match(r'(sharkM 1|sharkM 2|sharkM 3|sharkM 4|sharkM 5|sharkH 1|sharkH 2|sharkH 3|sharkH 4|sharkH 5|sharkM|sharkH|entrepreneurM|entrepreneurM 1|entrepreneurM 2|entrepreneurM 3|entrepreneurM 4|entrepreneurH|entrepreneurH 1|entrepreneurH 2|entrepreneurH 3|entrepreneurH 4|Alejandra Ríos - Woman|Alejandra Ríos - Female|Karla Berman - Woman|Karla Berman - Female|Amaury Vergara - Man|Amaury Vergara - Male|Marcus Dantus - Man|Marcus Dantus - Male|Marisa Lazo - Woman|Marisa Lazo - Female|Brian Requarth - Man|Brian Requarth - Male|Alejandro Litchi - Man|Alejandro Litchi - Male|Adriana Gallardo - Female|Adriana Gallardo - Woman|Oso Trava - Man|Oso Trava - Male|Speaker 1 - Woman|Speaker 1 - Female|Speaker 2 - Woman|Speaker 2 - Female|Speaker 3 - Woman|Speaker 3 - Female|Speaker 1 - Man|Speaker 1 - Male|Speaker 2 - Man|Speaker 2 - Male|Speaker 3 - Man|Speaker 3 - Male)', speaker)
    if speaker_type:
      dialogues[speaker_type.group()].append(content.strip())
      #print(dialogues)
      #  text separated in the dia logues dictionary
  shark_dialogues_M = dialogues['sharkM'] + dialogues['sharkM 1'] + dialogues['sharkM 2'] + dialogues['sharkM 3'] + dialogues['sharkM 4'] + dialogues['sharkM 5'] + dialogues['sharkM'] + dialogues['Alejandra Ríos - Woman'] + dialogues['Alejandra Ríos - Female'] + dialogues['Karla Berman - Woman']+dialogues['Karla Berman - Female']+ dialogues['Marisa Lazo - Woman']+ dialogues['Marisa Lazo - Female']+ dialogues['Adriana Gallardo - Woman']+ dialogues['Adriana Gallardo - Female']
  shark_dialogues_H = dialogues['sharkH'] + dialogues['sharkH 1'] + dialogues['sharkH 2'] + dialogues['sharkH 3'] + dialogues['sharkH 4'] + dialogues['sharkH 5'] + dialogues['sharkH'] + dialogues['Amaury Vergara - Man'] + dialogues['Amaury Vergara - Male']+ dialogues['Marcus Dantus - Man']+ dialogues['Marcus Dantus - Male']+dialogues['Oso Trava - Man']+ dialogues['Oso Trava - Male']+dialogues['Alejandro Litchi - Man']+ dialogues['Alejandro Litchi - Male']+dialogues['Brian Requarth - Man']+ dialogues['Brian Requarth - Male']

  # Pate as paragraph
  shark_paragraph_M = " ".join(shark_dialogues_M)
  shark_paragraph_H = " ".join(shark_dialogues_H)

  shark_paragraph_M = re.sub(r'[\r\n]', ' ', shark_paragraph_M)  # Replace line breaks with space
  shark_paragraph_H = re.sub(r'[\r\n]', ' ', shark_paragraph_H)  # Replace line breaks with space


  return shark_paragraph_M, shark_paragraph_H

def clean_text(text):
  stop_free = ' '.join([i for i in text.lower().split() if i not in stops])
  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
  normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())
  #removes all numbers
  final = "".join([i for i in normalized if not i.isdigit()])
  #eliminates double white spaces
  while "  " in final:
    final = final.replace("  ", " ")
  return final

def remove_stops(text, stops):
  #uses regex to remove all AC numbers
  text = re.sub(r"AC\/\d{1,4}\/\d{1,4}", "", text)
  #removes all stop words, including months
  words = text.split()
  final = []
  for word in words:
      if word not in stops:
          final.append(word)
  #reassembles the text without stop words
  final = " ".join(final)
  #removes all punctuation
  final = final.translate(str.maketrans("", "", string.punctuation))
  #removes all numbers
  final = "".join([i for i in final if not i.isdigit()])
  #eliminates double white spaces
  while "  " in final:
      final = final.replace("  ", " ")
  final = final.lower()
  return (final)


def remove_person_names(text):
    # Process the text with spaCy to identify named entities
    doc = nlp(text)
    result = []

    # Iterate through tokens in the processed doc
    for token in doc:
        # Skip tokens that are tagged as PERSON entities
        if token.ent_type_ == "PERSON":
            continue
        result.append(token.text)

    # Join the tokens back into a string
    return " ".join(result)

# Create a set of common names that might appear in lowercase
lowercase_names = {"jorge", 'alexa', 'valentina', 'inia', 'amauri', 'amaury', 'alejandra', 'alejandro',
                   "marcos", 'marcus', 'karla', 'oso', 'marisa',
                   "nt ", "nt", " nt", "mauricio", "adriana", "brian", "enrique",
                   "sofía", "guadalajara", "alec", "lichi", "alexei", "mario", "mauri", "\n\nAnd",
                   "ricardo", "diego", "gerardo", "zach", "benja", "adri", "mónica", "moni", "iliana", "deal",
                   "lisa", "beatriz", "adolfo", "peso", "pesos", "milion", "million", "millions", "dollar", "dollars",
                   "paola", "valeria", "litch", "okay", "natalia", "oxybenzon", "avobenzon", "homosal", "octisal",
                   "octocrylen", "methylparaben", "bemotrizinol", "retinyl", "palmit", "andres", "andrés",
                   "nayeli", "cecilia", "ernesto", "bruno", "sofia", "ignia", "julio", "paki", "kiblo", "huh",
                   "ll", "haha", "hehe", "lot", "juan", "josé", "jose", "rodrigo", "gabriela", "rogelio", "yes", "smes"
                   "oxybenzon avobenzon homosal octisal octocrylen palmit", "regina",
                   "peso", 'pesos', 'million','nicolás',
                   'juan', 'thank', 'ángel', 'daniel', 'coffee',
                   'offer','cream','cereal','osama','truck',
                   'ale', 'alberto', 'xochimilco', 'rich', 'homero', 'sandra', 'adri', 'kayak', 'kayaks', 'cindy', 'cofepris'}  # Add other names to this set as needed

def remove_person_lownames(text):
    # Process the text with spaCy to identify named entities
    doc = nlp(text)
    result = []

    # Iterate through tokens in the processed doc
    for token in doc:
        # Check if the token is a person entity or a common lowercase name
        if token.ent_type_ == "PERSON" or token.text.lower() in lowercase_names:
            continue
        result.append(token.text)

    # Join the tokens back into a string
    return " ".join(result)

def preprocesing_content(content):
    dialogues = defaultdict(list)
    # Pattern to match the speaker and text
    pattern = re.compile(r"\[(.*?)\]\n(.+?)(?=\[|$)", re.DOTALL)
    # Find all matches
    matches = pattern.findall(content)
    #print(matches)

    for speaker, content in matches:
      #print(speaker)
      #print(content)
      speaker_type = re.match(r'(sharkM 1|sharkM 2|sharkM 3|sharkM 4|sharkM 5|sharkH 1|sharkH 2|sharkH 3|sharkH 4|sharkH 5|sharkM|sharkH|entrepreneurM|entrepreneurM 1|entrepreneurM 2|entrepreneurM 3|entrepreneurM 4|entrepreneurH|entrepreneurH 1|entrepreneurH 2|entrepreneurH 3|entrepreneurH 4|Alejandra Ríos - Woman|Alejandra Ríos - Female|Karla Berman - Woman|Karla Berman - Female|Amaury Vergara - Man|Amaury Vergara - Male|Marcus Dantus - Man|Marcus Dantus - Male|Marisa Lazo - Woman|Marisa Lazo - Female|Brian Requarth - Man|Brian Requarth - Male|Alejandro Litchi - Man|Alejandro Litchi - Male|Adriana Gallardo - Female|Adriana Gallardo - Woman|Oso Trava - Man|Oso Trava - Male|Speaker 1 - Woman|Speaker 1 - Female|Speaker 2 - Woman|Speaker 2 - Female|Speaker 3 - Woman|Speaker 3 - Female|Speaker 1 - Man|Speaker 1 - Male|Speaker 2 - Man|Speaker 2 - Male|Speaker 3 - Man|Speaker 3 - Male)', speaker)
      if speaker_type:
        dialogues[speaker_type.group()].append(content.strip())
        #print(dialogues)
        #  text separated in the dialogues dictionary
    shark_dialogues_M = dialogues['sharkM'] + dialogues['sharkM 1'] + dialogues['sharkM 2'] + dialogues['sharkM 3'] + dialogues['sharkM 4'] + dialogues['sharkM 5'] + dialogues['sharkM'] + dialogues['Alejandra Ríos - Woman'] + dialogues['Alejandra Ríos - Female'] + dialogues['Karla Berman - Woman']+dialogues['Karla Berman - Female']+ dialogues['Marisa Lazo - Woman']+ dialogues['Marisa Lazo - Female']+ dialogues['Adriana Gallardo - Woman']+ dialogues['Adriana Gallardo - Female']
    shark_dialogues_H = dialogues['sharkH'] + dialogues['sharkH 1'] + dialogues['sharkH 2'] + dialogues['sharkH 3'] + dialogues['sharkH 4'] + dialogues['sharkH 5'] + dialogues['sharkH'] + dialogues['Amaury Vergara - Man'] + dialogues['Amaury Vergara - Male']+ dialogues['Marcus Dantus - Man']+ dialogues['Marcus Dantus - Male']+dialogues['Oso Trava - Man']+ dialogues['Oso Trava - Male']+dialogues['Alejandro Litchi - Man']+ dialogues['Alejandro Litchi - Male']+dialogues['Brian Requarth - Man']+ dialogues['Brian Requarth - Male']

    # Unir los diálogos en una sola cadena
    shark_paragraph_M = ' '.join(shark_dialogues_M)
    shark_paragraph_H = ' '.join(shark_dialogues_H)

    # Reemplazar saltos de línea por espacios
    cleaned_text_M = re.sub(r'[\r\n]', ' ', shark_paragraph_M)
    cleaned_text_H = re.sub(r'[\r\n]', ' ', shark_paragraph_H)

    # Convertir a minúsculas
    cleaned_text_M = cleaned_text_M.lower()
    cleaned_text_H = cleaned_text_H.lower()

    # Remover stopwords
    cleaned_text_M = remove_stops(cleaned_text_M, stops)
    cleaned_text_H = remove_stops(cleaned_text_H, stops)

    # Remover nombres de personas
    cleaned_text_M = remove_person_names(cleaned_text_M)
    cleaned_text_H = remove_person_names(cleaned_text_H)

    # Remover nombres comunes en minúsculas
    p_cleaned_text_M = remove_person_lownames(cleaned_text_M)
    p_cleaned_text_H = remove_person_lownames(cleaned_text_H)

    # Remover palabras con menos de 4 caracteres
    p_cleaned_text_M = ' '.join([word for word in p_cleaned_text_M.split() if len(word) >= 4])
    p_cleaned_text_H = ' '.join([word for word in p_cleaned_text_H.split() if len(word) >= 4])

    lemmas_H = [[token.lemma_ for token in sentence] for sentence in nlp(p_cleaned_text_H).sents]
    lemmas_H = ' '.join([item for sublist in lemmas_H for item in sublist])
    lemmas_H = remove_person_lownames(' '.join([word for word in re.split(r'\s+', lemmas_H) if len(word) > 3]))

    lemmas_M = [[token.lemma_ for token in sentence] for sentence in nlp(p_cleaned_text_M).sents]
    lemmas_M = ' '.join([item for sublist in lemmas_M for item in sublist])
    lemmas_M = remove_person_lownames(' '.join([word for word in re.split(r'\s+', lemmas_M) if len(word) > 3]))

    #print(lemmas_M)
    #print(lemmas_H)

    tokens_H = [[token.text for token in sentence] for sentence in nlp(lemmas_H).sents]
    stems_H = [[stemmer.stem(token) for token in sentence] for sentence in tokens_H]
    stems_H = ' '.join([item for sublist in stems_H for item in sublist])

    tokens_M = [[token.text for token in sentence] for sentence in nlp(lemmas_M).sents]
    stems_M = [[stemmer.stem(token) for token in sentence] for sentence in tokens_M]
    stems_M = ' '.join([item for sublist in stems_M for item in sublist])

    #print(stems_M)
    #print(stems_H)

    return shark_dialogues_M, shark_dialogues_H, p_cleaned_text_M, p_cleaned_text_H, lemmas_M, lemmas_H, stems_M, stems_H

path = 'Data/'
from keybert import KeyBERT
kw_model = KeyBERT()

all_dfs_01 = []
all_dfs_02 = []
all_dfs_03 = []

# Initialize lists before the loop
text_data_all_shark_h = []
text_data_all_shark_m = []
lemmas_h = []
lemmas_m = []
stems_h = []
stems_m = []
raw_m = []
raw_h = []

for filename in os.listdir(path):
  print(filename)
  if filename.endswith('.txt'):
    with open(os.path.join(path, filename), 'r') as f:
      content = f.read()
      rawM, rawH, topics_str_M, topics_str_H, lemmas_M, lemmas_H, stems_M, stems_H = preprocesing_content(content)
      print(topics_str_H)

      #Summary No Lemma No Stem
      topn = 1
      min_ngram = 1
      max_ngram = 15

      res_m01 = kw_model.extract_keywords(topics_str_M, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=True)

      res_h01 = kw_model.extract_keywords(topics_str_H, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=True)

      df_men01 = pd.DataFrame(res_h01, columns=['Relevant Topics for Shark Men 01', 'Score'])
      df_women01 = pd.DataFrame(res_m01, columns=['Relevant Topics for Shark Women 01', 'Score'])

      # Concatenate both DataFrames side by side
      df_iteration01 = pd.concat([df_men01, df_women01], axis=1)
      # Append the DataFrame to the list
      all_dfs_01.append(df_iteration01)

      #Summary Lemma
      topn = 1
      min_ngram = 1
      max_ngram = 15

      res_m02 = kw_model.extract_keywords(lemmas_M, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=True)

      res_h02 = kw_model.extract_keywords(lemmas_H, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=True)

      df_men02 = pd.DataFrame(res_h02, columns=['Relevant Topics for Shark Men 02', 'Score'])
      df_women02 = pd.DataFrame(res_m02, columns=['Relevant Topics for Shark Women 02', 'Score'])

      # Concatenate both DataFrames side by side
      df_iteration02 = pd.concat([df_men02, df_women02], axis=1)
      # Append the DataFrame to the list
      all_dfs_02.append(df_iteration02)

      #Summary Lemma and Stem
      res_m03 = kw_model.extract_keywords(stems_M, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=False)

      res_h03 = kw_model.extract_keywords(stems_H, keyphrase_ngram_range=(min_ngram, max_ngram),
                                          stop_words='english',
                                          diversity=0.80, top_n=topn, use_maxsum=False)

      df_men03 = pd.DataFrame(res_h03, columns=['Relevant Topics for Shark Men 03', 'Score'])
      df_women03 = pd.DataFrame(res_m03, columns=['Relevant Topics for Shark Women 03', 'Score'])

      # Concatenate both DataFrames side by side
      df_iteration03 = pd.concat([df_men03, df_women03], axis=1)
      all_dfs_03.append(df_iteration03)

      # Append data to the lists
      raw_m.append(rawM)
      raw_h.append(rawH)
      text_data_all_shark_h.append(topics_str_H)
      text_data_all_shark_m.append(topics_str_M)
      lemmas_h.append(lemmas_H)
      lemmas_m.append(lemmas_M)
      stems_h.append(stems_H)
      stems_m.append(stems_M)

# Now print the accumulated data
print(raw_m)
print(text_data_all_shark_m)
print(lemmas_m)
print(stems_m)

print(raw_h)
print(text_data_all_shark_h)
print(lemmas_h)
print(stems_h)

# Concatenate all DataFrames in the list vertically
df_01 = pd.concat(all_dfs_01, ignore_index=True)
df_02 = pd.concat(all_dfs_02, ignore_index=True)
df_03 = pd.concat(all_dfs_03, ignore_index=True)

# Commented out IPython magic to ensure Python compatibility.
# trying to incrporate coorpus
relevant_h = df_01['Relevant Topics for Shark Men 01'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_h]
print(key_phrases)
text_data_h = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_h)
corpus = [dictionary.doc2bow(text) for text in text_data_h]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)

# Commented out IPython magic to ensure Python compatibility.
# trying to incrporate coorpus
relevant_m = df_01['Relevant Topics for Shark Women 01'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_m]
print(key_phrases)
text_data_m = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_m)
corpus = [dictionary.doc2bow(text) for text in text_data_m]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)

# Commented out IPython magic to ensure Python compatibility.
# trying to incrporate coorpus
relevant_h = df_02['Relevant Topics for Shark Men 02'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_h]
print(key_phrases)
text_data_h = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_h)
corpus = [dictionary.doc2bow(text) for text in text_data_h]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)

# Commented out IPython magic to ensure Python compatibility.
# trying to incrporate coorpus
relevant_m = df_02['Relevant Topics for Shark Women 02'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_m]
print(key_phrases)
text_data_m = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_m)
corpus = [dictionary.doc2bow(text) for text in text_data_m]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)

# Commented out IPython magic to ensure Python compatibility.
# trying to incrporate coorpus
relevant_h = df_03['Relevant Topics for Shark Men 03'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_h]
print(key_phrases)
text_data_h = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_h)
corpus = [dictionary.doc2bow(text) for text in text_data_h]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)

# Commented out IPython magic to ensure Python compatibility.

# trying to incrporate coorpus
relevant_m = df_03['Relevant Topics for Shark Women 03'].unique()

# Prepare key phrases for LDA
key_phrases = [phrase for phrase in relevant_m]
print(key_phrases)
text_data_m = [phrase.split() for phrase in key_phrases]

# Create dictionary and corpus
dictionary = corpora.Dictionary(text_data_m)
corpus = [dictionary.doc2bow(text) for text in text_data_m]

num_topics = 3
lda = gensim.models.ldamodel.LdaModel
# %time ldamodel = lda(corpus, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0, random_state=42)

display(ldamodel.print_topics(num_topics=num_topics))

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display)